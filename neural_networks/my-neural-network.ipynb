{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4001ab44",
   "metadata": {},
   "source": [
    "# My Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9bd40aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f539b28",
   "metadata": {},
   "source": [
    "## Common Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b3b571e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "def546d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_sigmoid(z):\n",
    "    sig = sigmoid(z);\n",
    "    return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6bd5b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(a: NDArray[np.float64], y: NDArray[np.float64]):\n",
    "    if(len(a) != len(y)):\n",
    "        raise RuntimeError(f\"Length of calculated ({len(a)}) != Length of y {len(y)}\")\n",
    "    \n",
    "    return np.sum((a - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3acd72a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(20)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = np.array([1, 2, 3])\n",
    "two = np.array([2, 3, 4])\n",
    "\n",
    "np.dot(one, two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd90358",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d26a551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "    activation_func: Callable[[float], float];\n",
    "    w: NDArray[np.float64];\n",
    "    b: float\n",
    "    a: float\n",
    "    z_stored: float\n",
    "    delta: float\n",
    "    \n",
    "    prev_layer_activations: NDArray[np.float64]\n",
    "\n",
    "    def __init__(self, activation_func: Callable[[float], float], input_size: int):\n",
    "        self.activation_func = activation_func\n",
    "        self.w = np.random.randn(input_size) * 0.1\n",
    "        self.b = 0.0\n",
    "\n",
    "    def z(self, a_input_vec: NDArray[np.float64]) -> float:\n",
    "        self.z_stored = np.dot(self.w, a_input_vec) + self.b\n",
    "        return self.z_stored\n",
    "        \n",
    "    def get_activation(self, a_input_vec: NDArray[np.float64]) -> float:\n",
    "        self.prev_layer_activations = a_input_vec.copy();\n",
    "        self.a = self.activation_func(self.z(a_input_vec)) \n",
    "        return self.a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f0cc295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    neurons: List[Neuron] = []\n",
    "    input_size: int;\n",
    "    activations: List[float] = []\n",
    "    alpha = 0.1 # learning rate\n",
    "    \n",
    "    def __init__(self, width: int, activation_func: Callable[[float], float], input_size: int):\n",
    "        self.neurons = []\n",
    "        self.input_size = input_size\n",
    "\n",
    "        for i in range(width):\n",
    "            self.neurons.append(Neuron(activation_func, input_size))\n",
    "    \n",
    "    def get_activations(self, a: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        self.activations : List[float] = []\n",
    "        for n in self.neurons:\n",
    "            self.activations.append(n.get_activation(a))\n",
    "    \n",
    "        return np.array(self.activations)\n",
    "    \n",
    "    def update_weights_and_bias_by_y(self, y: NDArray[np.float64]):\n",
    "\n",
    "        for j, n in enumerate(self.neurons):\n",
    "            z = n.z_stored\n",
    "            a_j = n.a # The activation value of the current neuron\n",
    "            \n",
    "            dL_aj = 2 * (a_j - y[j]) \n",
    "            daj_dzj = deriv_sigmoid(z) \n",
    "\n",
    "            for k in range(len(n.prev_layer_activations)):\n",
    "                dzj_dwjk = n.prev_layer_activations[k]\n",
    "                n.w[k] -= self.alpha * dL_aj * daj_dzj * dzj_dwjk\n",
    "\n",
    "\n",
    "            n.b -= self.alpha * dL_aj * daj_dzj \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "fe72d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    layers: List[Layer]\n",
    "    alpha = 0.05\n",
    "    \n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        if len(layers) == 0:\n",
    "            raise RuntimeError(\"Layers should not be empty\")\n",
    "\n",
    "        self.layers = layers;\n",
    "\n",
    "    def evaluate(self, x_input: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        if(len(x_input) != self.layers[0].input_size):\n",
    "            raise RuntimeError(\"Input size layer mismatch\")\n",
    "            \n",
    "        # forward propogation\n",
    "        a = x_input\n",
    "        for layer in self.layers:\n",
    "            a = layer.get_activations(a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def train(self, x_input: NDArray[np.float64], y: NDArray[np.float64]):\n",
    "        # forward propagation\n",
    "        prediction = self.evaluate(x_input)\n",
    "\n",
    "        # backpropogation\n",
    "        all_deltas = [[] for _ in range(len(self.layers))]\n",
    "\n",
    "\n",
    "        output_deltas = []\n",
    "        for L, neuron in enumerate(self.layers[-1].neurons):\n",
    "            delta : float = 2 * (neuron.a - y[L]) * neuron.a * (1 - neuron.a)\n",
    "            output_deltas.append(delta)\n",
    "\n",
    "        all_deltas[-1] = output_deltas\n",
    "         \n",
    "        for L in reversed(range(len(self.layers) - 1)):\n",
    "            for i, neuron in enumerate(self.layers[L].neurons):\n",
    "\n",
    "                deriv_activation = neuron.a * (1 - neuron.a)\n",
    "\n",
    "                summation : float = 0.0\n",
    "                for j, next_neuron in enumerate(self.layers[L + 1].neurons):\n",
    "                    summation += next_neuron.w[i] * all_deltas[L + 1][j]\n",
    "\n",
    "                delta : float = deriv_activation * summation\n",
    "                all_deltas[L].append(delta)\n",
    "\n",
    "        # update weights\n",
    "        for L in reversed(range(len(self.layers))):\n",
    "            for i, neuron in enumerate(self.layers[L].neurons):\n",
    "                for j in range(len(neuron.w)):\n",
    "                    if L == 0:\n",
    "                        prev_activation = x_input[j]\n",
    "                    else:\n",
    "                        prev_activation = self.layers[L - 1].neurons[j].a\n",
    "                    neuron.w[j] -= self.alpha * all_deltas[L][i] * prev_activation\n",
    "                    neuron.b -= self.alpha * all_deltas[L][i]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9bbef",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "664c51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# Expected outputs (labels)\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "469d9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = Layer(width=4, activation_func=sigmoid, input_size=2)\n",
    "output_layer = Layer(width=1, activation_func=sigmoid, input_size=4)\n",
    "\n",
    "network = NeuralNetwork(layers=[hidden_layer, output_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2080300",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "68eee563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.9998972827455818\n",
      "Epoch 1000 - Loss: 0.9996693898807136\n",
      "Epoch 2000 - Loss: 0.9990363424160935\n",
      "Epoch 3000 - Loss: 0.9966779637195304\n",
      "Epoch 4000 - Loss: 0.9804012119739234\n",
      "Epoch 5000 - Loss: 0.7163377562734814\n",
      "Epoch 6000 - Loss: 0.12903391010319082\n",
      "Epoch 7000 - Loss: 0.04688575182005066\n",
      "Epoch 8000 - Loss: 0.026603891722236865\n",
      "Epoch 9000 - Loss: 0.018123502970270215\n",
      "Epoch 10000 - Loss: 0.013589689371613808\n",
      "Epoch 11000 - Loss: 0.010803540554730193\n",
      "Epoch 12000 - Loss: 0.008931484330769464\n",
      "Epoch 13000 - Loss: 0.007593201939469047\n",
      "Epoch 14000 - Loss: 0.006591992890099658\n",
      "Epoch 15000 - Loss: 0.005816467837993307\n",
      "Epoch 16000 - Loss: 0.005199061927128873\n",
      "Epoch 17000 - Loss: 0.004696523485616833\n",
      "Epoch 18000 - Loss: 0.004279936365207648\n",
      "Epoch 19000 - Loss: 0.0039292652661560455\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20000):  # or 10_000\n",
    "    for x, y_i in zip(X, y):\n",
    "        network.train(np.array(x), np.array(y_i))\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        predictions = np.array([network.evaluate(x)[0] for x in X])\n",
    "        total_loss = loss(predictions, y.flatten())\n",
    "        print(f\"Epoch {epoch} - Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b712cd",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f4eb6f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Input: [0 0] -> Output: 0.0254 | Prediction: 0 | Actual: 0\n",
      "[1] Input: [0 1] -> Output: 0.9704 | Prediction: 1 | Actual: 1\n",
      "[2] Input: [1 0] -> Output: 0.9701 | Prediction: 1 | Actual: 1\n",
      "[3] Input: [1 1] -> Output: 0.0349 | Prediction: 0 | Actual: 0\n"
     ]
    }
   ],
   "source": [
    "for i, x_input in enumerate(X):\n",
    "    output = network.evaluate(x_input)\n",
    "    prediction = 1 if output[0] > 0.5 else 0\n",
    "    print(f\"[{i}] Input: {x_input} -> Output: {output[0]:.4f} | Prediction: {prediction} | Actual: {y[i][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d37a4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] output of network: [0.02544445]\n",
      "prediction: 0 - actual: [0]\n",
      "\n",
      "[1] output of network: [0.97041946]\n",
      "prediction: 1 - actual: [1]\n",
      "\n",
      "[2] output of network: [0.97011157]\n",
      "prediction: 1 - actual: [1]\n",
      "\n",
      "[3] output of network: [0.03485307]\n",
      "prediction: 0 - actual: [0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, x_input in enumerate(X):\n",
    "    \n",
    "    output = network.evaluate(x_input)\n",
    "\n",
    "    prediction = 1 if output[0] > 0.5 else 0\n",
    "\n",
    "    print(f'[{i}] output of network: {output}')\n",
    "    print(f'prediction: {prediction} - actual: {y[i]}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
