{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4001ab44",
   "metadata": {},
   "source": [
    "# My Neural Network\n",
    "\n",
    "I built a Neural Network from Scratch, using Python and a whole lot of calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c067523",
   "metadata": {},
   "source": [
    "## Neural Network Overview \n",
    "\n",
    "**Neural Network Architecture (2-4-1)**\n",
    "- **Input**: 2 features\n",
    "- **Hidden Layer**: 4 neurons with sigmoid activation\n",
    "- **Output Layer**: 1 neuron with sigmoid activation\n",
    "- **Total Parameters**: (2Ã—4 + 4) + (4Ã—1 + 1) = 17 parameters\n",
    "\n",
    "**Trained**\n",
    "- XOR gate truth table (4 samples)\n",
    "- Non-linearly separable binary classification problem\n",
    "\n",
    "**Performance Results**\n",
    "- **Final Loss**: 0.0032 (Sum of Squared Errors)\n",
    "- **Accuracy**: 100% on all training samples\n",
    "- **Training**: 20,000 epochs with learning rate Î± = 0.05\n",
    "- **Convergence**: Successfully learned the XOR function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ec3e6",
   "metadata": {},
   "source": [
    "## Lessons Learned\n",
    "\n",
    "I am extremely **grateful** python libraries such as TensorFlow and PyTorch\n",
    "\n",
    "In retrospect, I would **utilize more Linear Algebra**. I misunderstood how neurons worked in neural networks, and made a separate `Neuron` class for them, which my `Layer` class instantiated. Rather, I should have just built out 2D matrixes in the `Layer` class, and just indexed into them and used np vector functions such as `np.sum()` or `np.dot()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9bd40aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f539b28",
   "metadata": {},
   "source": [
    "## Common Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b7507",
   "metadata": {},
   "source": [
    "**Sigmoid**\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b3b571e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b67cf",
   "metadata": {},
   "source": [
    "**Derivative of Sigmoid**\n",
    "$$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "def546d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_sigmoid(z):\n",
    "    sig = sigmoid(z);\n",
    "    return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4af84",
   "metadata": {},
   "source": [
    "**Loss Function (Sum of Squared Errors)**\n",
    "$$L(a, y) = \\sum_{i=1}^{n} (a_i - y_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6bd5b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(a: NDArray[np.float64], y: NDArray[np.float64]):\n",
    "    if(len(a) != len(y)):\n",
    "        raise RuntimeError(f\"Length of calculated ({len(a)}) != Length of y {len(y)}\")\n",
    "    \n",
    "    return np.sum((a - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd90358",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff81504",
   "metadata": {},
   "source": [
    "**Neuron Activation Function**\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "$$a = \\sigma(z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "    activation_func: Callable[[float], float];\n",
    "    w: NDArray[np.float64];\n",
    "    b: float\n",
    "    a: float\n",
    "    z_stored: float\n",
    "    \n",
    "    prev_layer_activations: NDArray[np.float64]\n",
    "\n",
    "    def __init__(self, activation_func: Callable[[float], float], input_size: int):\n",
    "        self.activation_func = activation_func\n",
    "        self.w = np.random.randn(input_size) * 0.1\n",
    "        self.b = 0.0\n",
    "\n",
    "    def z(self, a_input_vec: NDArray[np.float64]) -> float:\n",
    "        self.z_stored = np.dot(self.w, a_input_vec) + self.b\n",
    "        return self.z_stored\n",
    "        \n",
    "    def get_activation(self, a_input_vec: NDArray[np.float64]) -> float:\n",
    "        self.prev_layer_activations = a_input_vec.copy();\n",
    "        self.a = self.activation_func(self.z(a_input_vec)) \n",
    "        return self.a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    neurons: List[Neuron] = []\n",
    "    input_size: int;\n",
    "    activations: List[float] = []\n",
    "    alpha = 0.1 # learning rate\n",
    "    \n",
    "    def __init__(self, width: int, activation_func: Callable[[float], float], input_size: int):\n",
    "        self.neurons = []\n",
    "        self.input_size = input_size\n",
    "\n",
    "        for i in range(width):\n",
    "            self.neurons.append(Neuron(activation_func, input_size))\n",
    "    \n",
    "    def get_activations(self, a: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        self.activations : List[float] = []\n",
    "        for n in self.neurons:\n",
    "            self.activations.append(n.get_activation(a))\n",
    "    \n",
    "        return np.array(self.activations)\n",
    "    \n",
    "    # Wrong approach to update weight + biases\n",
    "    # Need to find delta to see how much error each neuron is responsible for\n",
    "    def _update_weights_and_bias_by_y(self, y: NDArray[np.float64]):\n",
    "\n",
    "        for j, n in enumerate(self.neurons):\n",
    "            z = n.z_stored\n",
    "            a_j = n.a # The activation value of the current neuron\n",
    "            \n",
    "            dL_aj = 2 * (a_j - y[j]) \n",
    "            daj_dzj = deriv_sigmoid(z) \n",
    "\n",
    "            for k in range(len(n.prev_layer_activations)):\n",
    "                dzj_dwjk = n.prev_layer_activations[k]\n",
    "                n.w[k] -= self.alpha * dL_aj * daj_dzj * dzj_dwjk\n",
    "\n",
    "\n",
    "            n.b -= self.alpha * dL_aj * daj_dzj "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2c95c",
   "metadata": {},
   "source": [
    "**Backpropagation Algorithm**\n",
    "\n",
    "**Step 1: Output Layer Delta**\n",
    "$$\\delta_i^{(L)} = \\frac{\\partial L}{\\partial a_i^{(L)}} \\cdot \\sigma'(z_i^{(L)})$$\n",
    "\n",
    "**Step 2: Hidden Layer Delta (Backward Propagation)**\n",
    "$$\\delta_i^{(l)} = \\sigma'(z_i^{(l)}) \\cdot \\sum_{j} w_{ji}^{(l+1)} \\delta_j^{(l+1)}$$\n",
    "\n",
    "**Step 3: Weight Updates**\n",
    "$$w_{ij}^{(l)} = w_{ij}^{(l)} - \\alpha \\cdot \\delta_j^{(l)} \\cdot a_i^{(l-1)}$$\n",
    "**Step 4: Bias Updates**\n",
    "$$b_j^{(l)} = b_j^{(l)} - \\alpha \\cdot \\delta_j^{(l)}$$\n",
    "\n",
    "\n",
    "Where:\n",
    "$$ \\frac{\\partial L}{\\partial a_i^{(L)}} = 2(a_i^{(L)} - y_i)$$\n",
    "\n",
    "$$\\sigma'(z_i^{(l)}) = a_i^{(l)} \\cdot (1 - a_i^{(l)})$$\n",
    "\n",
    "**Notation:**\n",
    "- $L$ = output layer\n",
    "- $l$ = current layer index\n",
    "- $\\delta_i^{(l)}$ = error signal for neuron $i$ in layer $l$\n",
    "- $w_{ij}^{(l)}$ = weight from neuron $i$ in layer $(l-1)$ to neuron $j$ in layer $l$\n",
    "- $a_i^{(l)}$ = activation of neuron $i$ in layer $l$\n",
    "- $\\alpha$ = learning rate\n",
    "- $y_i$ = target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    layers: List[Layer]\n",
    "    alpha = 0.05\n",
    "    \n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        if len(layers) == 0:\n",
    "            raise RuntimeError(\"Layers should not be empty\")\n",
    "\n",
    "        self.layers = layers;\n",
    "\n",
    "    def evaluate(self, x_input: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        if(len(x_input) != self.layers[0].input_size):\n",
    "            raise RuntimeError(\"Input size layer mismatch\")\n",
    "            \n",
    "        # forward propogation\n",
    "        a = x_input\n",
    "        for layer in self.layers:\n",
    "            a = layer.get_activations(a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def train(self, x_input: NDArray[np.float64], y: NDArray[np.float64]):\n",
    "        # forward propagation\n",
    "        self.evaluate(x_input)\n",
    "\n",
    "        # backpropogation\n",
    "        all_deltas = [[] for _ in range(len(self.layers))]\n",
    "\n",
    "\n",
    "        output_deltas = []\n",
    "        for L, neuron in enumerate(self.layers[-1].neurons):\n",
    "            delta : float = 2 * (neuron.a - y[L]) * neuron.a * (1 - neuron.a)\n",
    "            output_deltas.append(delta)\n",
    "\n",
    "        all_deltas[-1] = output_deltas\n",
    "         \n",
    "        for L in reversed(range(len(self.layers) - 1)):\n",
    "            for i, neuron in enumerate(self.layers[L].neurons):\n",
    "\n",
    "                deriv_activation = neuron.a * (1 - neuron.a)\n",
    "\n",
    "                summation : float = 0.0\n",
    "                for j, next_neuron in enumerate(self.layers[L + 1].neurons):\n",
    "                    summation += next_neuron.w[i] * all_deltas[L + 1][j]\n",
    "\n",
    "                delta : float = deriv_activation * summation\n",
    "                all_deltas[L].append(delta)\n",
    "\n",
    "        # update weights\n",
    "        for L in reversed(range(len(self.layers))):\n",
    "            for i, neuron in enumerate(self.layers[L].neurons):\n",
    "                for j in range(len(neuron.w)):\n",
    "                    if L == 0:\n",
    "                        prev_activation = x_input[j]\n",
    "                    else:\n",
    "                        prev_activation = self.layers[L - 1].neurons[j].a\n",
    "                    neuron.w[j] -= self.alpha * all_deltas[L][i] * prev_activation\n",
    "                    neuron.b -= self.alpha * all_deltas[L][i]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9bbef",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5974a",
   "metadata": {},
   "source": [
    "### Training Data\n",
    "\n",
    "XOR Gate\n",
    "\n",
    "\n",
    "\n",
    "| $\\mathbf{x_1}$ | $\\mathbf{x_2}$ | $\\mathbf{y}$ |\n",
    "|----------------|----------------|--------------|\n",
    "|       0        |       0        |      0       |\n",
    "|       0        |       1        |      1       |\n",
    "|       1        |       0        |      1       |\n",
    "|       1        |       1        |      0       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "664c51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# Expected outputs (labels)\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e80fcc",
   "metadata": {},
   "source": [
    "**Neural Network Architecture (2-4-1)**\n",
    "- **Input**: 2 features\n",
    "- **Hidden Layer**: 4 neurons with sigmoid activation\n",
    "- **Output Layer**: 1 neuron with sigmoid activation\n",
    "- **Total Parameters**: (2Ã—4 + 4) + (4Ã—1 + 1) = 17 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "469d9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = Layer(width=4, activation_func=sigmoid, input_size=2)\n",
    "output_layer = Layer(width=1, activation_func=sigmoid, input_size=4)\n",
    "\n",
    "network = NeuralNetwork(layers=[hidden_layer, output_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2080300",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "68eee563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.9998972827455818\n",
      "Epoch 1000 - Loss: 0.9996693898807136\n",
      "Epoch 2000 - Loss: 0.9990363424160935\n",
      "Epoch 3000 - Loss: 0.9966779637195304\n",
      "Epoch 4000 - Loss: 0.9804012119739234\n",
      "Epoch 5000 - Loss: 0.7163377562734814\n",
      "Epoch 6000 - Loss: 0.12903391010319082\n",
      "Epoch 7000 - Loss: 0.04688575182005066\n",
      "Epoch 8000 - Loss: 0.026603891722236865\n",
      "Epoch 9000 - Loss: 0.018123502970270215\n",
      "Epoch 10000 - Loss: 0.013589689371613808\n",
      "Epoch 11000 - Loss: 0.010803540554730193\n",
      "Epoch 12000 - Loss: 0.008931484330769464\n",
      "Epoch 13000 - Loss: 0.007593201939469047\n",
      "Epoch 14000 - Loss: 0.006591992890099658\n",
      "Epoch 15000 - Loss: 0.005816467837993307\n",
      "Epoch 16000 - Loss: 0.005199061927128873\n",
      "Epoch 17000 - Loss: 0.004696523485616833\n",
      "Epoch 18000 - Loss: 0.004279936365207648\n",
      "Epoch 19000 - Loss: 0.0039292652661560455\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20000):  # or 10_000\n",
    "    for x, y_i in zip(X, y):\n",
    "        network.train(np.array(x), np.array(y_i))\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        predictions = np.array([network.evaluate(x)[0] for x in X])\n",
    "        total_loss = loss(predictions, y.flatten())\n",
    "        print(f\"Epoch {epoch} - Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e7022",
   "metadata": {},
   "source": [
    "A *beautiful* **Loss of 0.0039** ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©ðŸ¤©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b712cd",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f4eb6f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Input: [0 0] -> Output: 0.0254 | Prediction: 0 | Actual: 0\n",
      "[1] Input: [0 1] -> Output: 0.9704 | Prediction: 1 | Actual: 1\n",
      "[2] Input: [1 0] -> Output: 0.9701 | Prediction: 1 | Actual: 1\n",
      "[3] Input: [1 1] -> Output: 0.0349 | Prediction: 0 | Actual: 0\n"
     ]
    }
   ],
   "source": [
    "for i, x_input in enumerate(X):\n",
    "    output = network.evaluate(x_input)\n",
    "    prediction = 1 if output[0] > 0.5 else 0\n",
    "    print(f\"[{i}] Input: {x_input} -> Output: {output[0]:.4f} | Prediction: {prediction} | Actual: {y[i][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81effd",
   "metadata": {},
   "source": [
    "And as you can see, my neural network learned. Im so proud ðŸ¥²"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
